<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <title>Project: The Engine of Learning</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.em%22 font-size=%2290%22>ðŸ§ </text></svg>">
  <style>
    body { font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 40px 20px; line-height: 1.7; color: #333; background: #fff; }
    h1 { font-size: 2.4em; color: #1a1a1a; margin-bottom: 0.2em; font-weight: 700; }
    h2 { font-size: 1.6em; color: #2c3e50; margin-top: 2.5em; margin-bottom: 1.2em; font-weight: 600; border-bottom: 2px solid #3498db; padding-bottom: 0.3em; }
    h3 { font-size: 1.2em; color: #34495e; margin-bottom: 0.5em; font-weight: 600; }
    p, ul, li { margin-bottom: 1em; font-size: 1.05em; text-align: justify;}
    a { color: #3498db; text-decoration: none; font-weight: 500; }
    a:hover { text-decoration: underline; }
    .main-header { text-align: center; margin-bottom: 50px; padding-bottom: 30px; border-bottom: 1px solid #eee; }
    .back-link { display: block; text-align: center; margin-bottom: 40px; font-weight: 600; }
    figure { margin: 2.5em 0; text-align: center; }
    figcaption { font-size: 0.9em; color: #7f8c8d; margin-top: 0.8em; line-height: 1.5; max-width: 80%; margin-left: auto; margin-right: auto;}
    img { max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }
    pre { background-color: #2d2d2d; color: #f8f8f2; padding: 1.5em; border-radius: 8px; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;}
    pre code { background: none; padding: 0; }
    .analysis-block { margin-top: 1.5em; padding-left: 1.5em; border-left: 3px solid #3498db; font-style: italic; }
    .math-block { margin: 2em auto; padding: 1.5em; background-color: #f9f9f9; border-left: 4px solid #3498db; }
  </style>
</head>
<body>

  <header class="main-header">
    <h1>The Engine of Learning</h1>
    <p style="font-size: 1.15em; color: #555; margin: 0.5em 0 1.5em 0; text-align: center;">A "From-Scratch" Guide to Backpropagation</p>
  </header>

  <a href="../blog.html" class="back-link">&larr; Back to Projects & Writing Hub</a>

  <figure>
    <img src="https://i.imgur.com/uXXE12m.png" alt="A diagram showing backpropagation in a neural network">
    <figcaption>Backpropagation is a message-passing algorithm where an error signal flows backward from the output, allowing every parameter in the network to know how to adjust itself to improve the overall prediction.</figcaption>
  </figure>

  <section id="introduction">
    <h2>The Million-Knob Problem</h2>
    <p>
      Imagine a machine with a million knobs. Your goal is to tune these knobs to produce a specific, desired output. After your first attempt, the output is wrong. How do you decide which knob to turn, in which direction, and by how much? Turning them randomly is hopeless. This is the challenge faced by every neural networkâ€”a vast landscape of parameters (weights and biases) that must be tuned to minimize error.
    </p>
    <p>
      The solution to this "credit assignment" problem is an elegant and powerful algorithm called **Backpropagation**. It is, without exaggeration, the engine that powers modern deep learning. This post will demystify backpropagation by building it from the ground up, starting with a single neuron and scaling up to a full, vectorized network.
    </p>
  </section>

  <section id="concepts">
    <h2>The Core Idea: A Cascade of Blame</h2>
    <p>
      At its heart, backpropagation is a method for efficiently calculating the gradient of the loss function with respect to every parameter in the network. The gradient tells us the "slope" of the loss landscapeâ€”the direction of steepest error increase. To learn, we simply take a small step in the opposite direction.
    </p>
    <p>
      The algorithm works by propagating the error signal backward, layer by layer. It starts at the final output and asks, "How did this prediction contribute to the total error?" It then takes that answer and moves to the previous layer, asking, "And how did this layer's outputs contribute to the error of the *next* layer?" This cascade of blame continues until it reaches the very first layer, providing every parameter with a precise gradient that quantifies its contribution to the overall error.
    </p>
    <div class="analysis-block">
        <p><strong>Analogy:</strong> Imagine a multi-stage rocket launch fails. Engineers analyze the final stage first to see what went wrong. Based on those findings, they analyze the stage before it, and so on, tracing the source of the failure backward in time. Backpropagation does the same, but with mathematical error instead of mechanical failure.</p>
    </div>
  </section>
  
  <section id="math">
    <h2>The Mathematics: A Chain Reaction of Derivatives</h2>
    <p>
      The mechanism behind backpropagation is calculus's **chain rule**. It allows us to calculate the derivative of a composite function. Since a neural network is just a giant, nested function, the chain rule is the perfect tool.
    </p>

    <h3>Level 1: A Single Linear Neuron</h3>
    <p>Let's start with the simplest possible case: one input $x$, one weight $w$, one bias $b$, and a linear prediction $y_{pred}$. The loss $L$ is measured by Mean Squared Error (MSE).</p>
    <div class="math-block">
      <p>
        <strong>Forward Pass:</strong><br>
        1. Prediction: $y_{pred} = w \cdot x + b$<br>
        2. Loss: $L = \frac{1}{2}(y_{true} - y_{pred})^2$
      </p>
      <p>
        <strong>Backward Pass (Goal: find $\frac{\partial L}{\partial w}$):</strong><br>
        We use the chain rule: $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial w}$
      </p>
      <p>
        1. $\frac{\partial L}{\partial y_{pred}} = y_{pred} - y_{true}$ &nbsp; (The initial error signal)<br>
        2. $\frac{\partial y_{pred}}{\partial w} = x$ &nbsp; (The local derivative)
      </p>
      <p>
        <strong>Result:</strong> $\frac{\partial L}{\partial w} = (y_{pred} - y_{true}) \cdot x$
      </p>
    </div>

    <h3>Level 2: Adding a Non-Linear Activation</h3>
    <p>To learn complex patterns, we need non-linear activation functions like Sigmoid ($\sigma$). This adds another link to our chain.</p>
    <div class="math-block">
      <p>
        <strong>Forward Pass:</strong><br>
        1. Pre-activation: $z = w \cdot x + b$<br>
        2. Activation: $y_{pred} = \sigma(z)$<br>
        3. Loss: $L = \frac{1}{2}(y_{true} - y_{pred})^2$
      </p>
      <p>
        <strong>Backward Pass (Goal: find $\frac{\partial L}{\partial w}$):</strong><br>
        The chain rule now has three parts: $\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial z} \cdot \frac{\partial z}{\partial w}$
      </p>
      <p>
        1. $\frac{\partial L}{\partial y_{pred}} = y_{pred} - y_{true}$<br>
        2. $\frac{\partial y_{pred}}{\partial z} = \sigma'(z)$ &nbsp; (Derivative of the activation function)<br>
        3. $\frac{\partial z}{\partial w} = x$
      </p>
    </div>
    
    <h3>Level 3: Scaling to Multi-Layer, Multi-Node Networks</h3>
    <p>
      When we scale up, two things happen:
    </p>
    <ul>
      <li><strong>Multi-Layer:</strong> The chain of derivatives gets longer. The gradient flowing out of one layer becomes the incoming error signal for the layer before it. For a 2-layer network, the gradient for a first-layer weight $w_1$ becomes:
        $\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y_{pred}} \cdot \frac{\partial y_{pred}}{\partial z_2} \cdot \frac{\partial z_2}{\partial h} \cdot \frac{\partial h}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1}$
      </li>
      <li><strong>Multi-Node:</strong> If a neuron's output goes to multiple neurons in the next layer, its "blame" is the <strong>sum</strong> of the gradients coming back from all those paths. This is how a neuron correctly accounts for its total influence on the final loss.</li>
    </ul>
    <p>
      Doing these calculations one by one is computationally impossible. This is why we use **vectorization**. The entire forward and backward pass for a full layer of neurons can be computed with a few efficient matrix multiplications.
    </p>

  </section>

  <section id="exercises">
    <h2>Exercises for the Reader</h2>
    <p>Test your understanding by manually calculating the gradients for the following scenarios.</p>

    <h3>Problem 1: Simple Linear Neuron</h3>
    <pre><code>Setup:
Input: x = 4
Weight: w = -2
Bias: b = 3
True Label: y_true = -3
Loss: L = 1/2 * (y_true - y_pred)^2

Task: Calculate the gradients dL/dw and dL/db.</code></pre>

    <h3>Problem 2: Neuron with Sigmoid</h3>
    <pre><code>Setup:
Input: x = 1
Weight: w = 2
Bias: b = -1
True Label: y_true = 1
Model: z = w*x + b,  y_pred = sigmoid(z)
Loss: L = 1/2 * (y_true - y_pred)^2

Task: Calculate dL/dw. You will need a calculator for sigmoid(1) â‰ˆ 0.731.</code></pre>
  </section>

  <section id="solutions">
    <h2>Solutions</h2>
    
    <h3>Solution to Problem 1</h3>
    <pre><code>1. Forward Pass:
   y_pred = (-2 * 4) + 3 = -5
   L = 1/2 * (-3 - (-5))^2 = 1/2 * (2)^2 = 2

2. Backward Pass:
   Error Signal (dL/d_ypred) = y_pred - y_true = -5 - (-3) = -2
   dL/dw = (Error Signal) * x = (-2) * 4 = -8
   dL/db = (Error Signal) * 1 = (-2) * 1 = -2</code></pre>

    <h3>Solution to Problem 2</h3>
    <pre><code>1. Forward Pass:
   z = (2 * 1) + (-1) = 1
   y_pred = sigmoid(1) â‰ˆ 0.731
   L = 1/2 * (1 - 0.731)^2 â‰ˆ 0.036

2. Backward Pass:
   dL/d_ypred = y_pred - y_true = 0.731 - 1 = -0.269
   d_ypred/dz = sigmoid'(z) = y_pred * (1 - y_pred) = 0.731 * (1 - 0.731) â‰ˆ 0.197
   dz/dw = x = 1
   
   dL/dw = dL/d_ypred * d_ypred/dz * dz/dw
   dL/dw â‰ˆ (-0.269) * (0.197) * (1) â‰ˆ -0.053</code></pre>
  </section>
  
  <section id="conclusion">
    <h2>Conclusion: The Algorithm That Unlocked Deep Learning</h2>
    <p>
      Backpropagation is not magic. It is the methodical and recursive application of the chain rule from calculus, made computationally feasible through vectorization. By providing an efficient way to solve the "credit assignment" problem, it transformed neural networks from a theoretical curiosity into the powerful engine behind the deep learning revolution. Understanding this algorithm from the ground up reveals the elegant mathematical core that allows machines to learn.
    </p>
  </section>

  <section id="code">
      <h2>Code & Resources</h2>
      <p style="text-align: center;">A complete, modular "from-scratch" implementation of a neural network trained on MNIST, based on the principles discussed here, is available on GitHub.</p>
      <p style="text-align: center; margin-top: 1.5em;">
          <a href="https://github.com/CedricCaruzzo/Backpropagation-from-scratch" target="_blank" style="background-color: #2c3e50; color: white; padding: 12px 25px; border-radius: 8px; font-weight: 600; display: inline-block;">View on GitHub</a>
      </p>
  </section>
  
  <a href="../blog.html" class="back-link" style="margin-top: 50px;">&larr; Back to Projects & Writing Hub</a>

</body>
</html>