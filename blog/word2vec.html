<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
  <title>Project: The Geometry of Words</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.em%22 font-size=%2290%22>ðŸ¤–</text></svg>">
  <style>
    body { font-family: "Inter", -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto; padding: 40px 20px; line-height: 1.7; color: #333; background: #fff; }
    h1 { font-size: 2.4em; color: #1a1a1a; margin-bottom: 0.2em; font-weight: 700; }
    h2 { font-size: 1.6em; color: #2c3e50; margin-top: 2.5em; margin-bottom: 1.2em; font-weight: 600; border-bottom: 2px solid #3498db; padding-bottom: 0.3em; }
    h3 { font-size: 1.2em; color: #34495e; margin-bottom: 0.5em; font-weight: 600; }
    p, ul, li { margin-bottom: 1em; font-size: 1.05em; text-align: justify;}
    a { color: #3498db; text-decoration: none; font-weight: 500; }
    a:hover { text-decoration: underline; }
    .main-header { text-align: center; margin-bottom: 50px; padding-bottom: 30px; border-bottom: 1px solid #eee; }
    .back-link { display: block; text-align: center; margin-bottom: 40px; font-weight: 600; }
    figure { margin: 2.5em 0; text-align: center; }
    figcaption { font-size: 0.9em; color: #7f8c8d; margin-top: 0.8em; line-height: 1.5; max-width: 80%; margin-left: auto; margin-right: auto;}
    img { max-width: 100%; height: auto; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1); }
    pre { background-color: #2d2d2d; color: #f8f8f2; padding: 1.5em; border-radius: 8px; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;}
    pre code { background: none; padding: 0; }
    .analysis-block { margin-top: 1.5em; padding-left: 1.5em; border-left: 3px solid #3498db; font-style: italic; }
  </style>
</head>
<body>

  <header class="main-header">
    <h1>The Geometry of Words</h1>
    <p style="font-size: 1.15em; color: #555; margin: 0.5em 0 1.5em 0; text-align: center;">A "From-Scratch" Word2Vec Playground in PyTorch</p>
  </header>

  <a href="../blog.html" class="back-link">&larr; Back to Projects & Writing Hub</a>

  <figure>
    <img src="../img/word2vec/word2vec.jpeg" alt="A conceptual figure showing what we expect from word2vec manifold">
    <figcaption>A conceptual visualization of the learned word embeddings. Semantically similar words cluster together, revealing the geometric structure of language.</figcaption>
  </figure>

  <section id="introduction">
    <h2>How Many Ideas Can You Fit into 300 Numbers?</h2>
    <p>
      Word embeddings are dense vector representations of words, but have you ever considered their limits? If a 300-dimensional vector represents a word's "concept," how many truly unique concepts can it hold? If "unique" means perfectly orthogonal, the answer is just 300. Yet vocabularies contain tens of thousands of words. How can so few dimensions capture so much meaning?
    </p>
    <p>
      The magic lies in high-dimensional geometry. In a high-dimensional space, you can pack an astronomical number of vectors that are *almost* orthogonal. This project demystifies the process by building Word2Vec (specifically, the Skip-Gram with Negative Sampling model) from the ground up, revealing how an algorithm can learn to navigate this vast "meaning space."
    </p>
  </section>

  <section id="concepts">
    <h2>The Core Idea: Learning from Context</h2>
    <p>
      The Skip-Gram model works on a simple predictive task: **given a target word, predict its surrounding context words.** For example, given the word `fox`, the model learns a vector representation that is good at predicting nearby words like `quick`, `brown`, `jumps`, and `over`.
    </p>
    <h3>The Efficiency Problem & The Solution: Negative Sampling</h3>
    <p>
      A naive approach would require a final softmax layer over the entire vocabularyâ€”a massive computational bottleneck. The key optimization is **Negative Sampling**, which reframes the task into a simple binary classification problem. Instead of predicting context words, the model learns to distinguish between true `(target, context)` pairs (positive examples) and randomly generated fake pairs (negative examples). This is vastly more efficient.
    </p>
  </section>
  
  <section id="results">
    <h2>Analyzing the Results: Successes and "Correct" Errors</h2>
    <p>
      The true test of the model is to inspect the nearest neighbors of various words. The results show both clear successes in capturing semantic meaning and fascinating "errors" that reveal biases in the training data.
    </p>
    <h3>Clear Successes</h3>
    <pre><code>Words closest to 'king':
  - kings      (similarity: 0.6130)
  - throne     (similarity: 0.5964)
  - reigned    (similarity: 0.5740)
  - son        (similarity: 0.5612)
  - prince     (similarity: 0.5282)

Words closest to 'computer':
  - computers  (similarity: 0.7196)
  - hardware   (similarity: 0.6161)
  - machines   (similarity: 0.6085)
  - software   (similarity: 0.5428)</code></pre>
    <div class="analysis-block">
        <p><strong>Analysis:</strong> The model successfully learned the semantic concepts, grouping words related to royalty for `king` and technology for `computer`.</p>
    </div>

    <h3>Fascinating Artifacts: When "Wrong" is Right</h3>
    <pre><code>Words closest to 'history':
  - references (similarity: 0.5308)
  - links      (similarity: 0.5181)
  - article    (similarity: 0.4784)

Words closest to 'france':
  - french     (similarity: 0.6270)
  - italy      (similarity: 0.5540)
  - partements (similarity: 0.5381)
  - partement  (similarity: 0.5164)</code></pre>
    <div class="analysis-block">
        <p><strong>Analysis:</strong> These "errors" are actually correct learnings of corpus artifacts. The model didn't learn the abstract concept of `history`; it learned the structural context of a Wikipedia article, where "history" sections are followed by "references" and "links." Similarly, it correctly associated `france` with `partements`, a tokenization artifact from the French phrase "dÃ©partements d'outre-mer" in the pre-cleaned training data.</p>
    </div>
  </section>

  <section id="conclusion">
    <h2>The Limits of Static and the Leap to Dynamic</h2>
    <p>
        This project highlights both the power and the limitations of static embeddings. A word like "bank" will always have a single vector, regardless of whether it refers to a river bank or a financial institution. This is the trade-off. What if an embedding wasn't static? What if "bank" could move closer to "money" or "river" depending on the sentence it's in?
    </p>
    <p>
        That leapâ€”from a frozen dictionary to a living, context-aware language modelâ€”is exactly what architectures like the Transformer make possible, paving the way for the next generation of natural language understanding.
    </p>
  </section>

  <section id="code">
      <h2>Code & Resources</h2>
      <p style="text-align: center;">This project is an open-source playground. The code is heavily commented and modularized to encourage exploration into the geometry of language.</p>
      <p style="text-align: center; margin-top: 1.5em;">
          <a href="https://github.com/CedricCaruzzo/minimal-word2vec" target="_blank" style="background-color: #2c3e50; color: white; padding: 12px 25px; border-radius: 8px; font-weight: 600; display: inline-block;">View on GitHub</a>
      </p>
  </section>
  
  <a href="../blog.html" class="back-link" style="margin-top: 50px;">&larr; Back to Projects & Writing Hub</a>

</body>
</html>
